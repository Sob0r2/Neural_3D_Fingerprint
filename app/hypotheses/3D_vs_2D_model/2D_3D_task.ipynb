{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ed384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error') \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from model_2D.model_2D import GNNFingerprint2D\n",
    "from model_2D.utils import smiles_to_2D_fp\n",
    "\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from model.model import GNNFingerprint3D\n",
    "from hypotheses.compare_with_other_fingerprints.utils import smiles_to_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac28833",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getenv(\"DATA_PATH\")\n",
    "models_path = os.getenv(\"MODELS_PATH\")\n",
    "\n",
    "columns = [\"mu\", \"zpve\", \"energy_U0\", \"rcA\", \"rcB\", \"rcC\", \"Cv\", \"alpha\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159f154",
   "metadata": {},
   "source": [
    "### 2D Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bacf586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, ki_threshold):\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    df['Activity'] = df['Standard Value'].apply(lambda x: 1 if x < ki_threshold else 0)\n",
    "\n",
    "    df = df[['Smiles', 'Activity']].dropna()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['Smiles'], df['Activity'], test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d47c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(X_train, y_train, X_test, y_test, task = \"c\"):\n",
    "    s_scaler = StandardScaler()\n",
    "    X_train = s_scaler.fit_transform(X_train)\n",
    "    X_test = s_scaler.transform(X_test)\n",
    "\n",
    "    # PCA to 167 dim\n",
    "    pca = PCA(n_components=167)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    if task == \"c\":\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    else:\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        print(\"Regression Metrics:\")\n",
    "        print(f\"MAE:  {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "        print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "        print(f\"RÂ²:   {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84153bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp\\ipykernel_15404\\596744975.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fingerprint_model.load_state_dict(torch.load(os.path.join(models_path, \"GNN_MUCH_MORE_WEIGHT_3D.pth\")))\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp\\ipykernel_15404\\596744975.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_2D.load_state_dict(torch.load(os.path.join(models_path, \"2D_GNN_MODEL.pth\")))\n"
     ]
    }
   ],
   "source": [
    "fingerprint_model = GNNFingerprint3D(13, 5)\n",
    "fingerprint_model.load_state_dict(torch.load(os.path.join(models_path, \"GNN_MORE_WEIGHT_3D.pth\")))\n",
    "fingerprint_model = fingerprint_model.to(\"cuda\")\n",
    "fingerprint_model.eval()\n",
    "\n",
    "model_2D = GNNFingerprint2D(14)\n",
    "model_2D.load_state_dict(torch.load(os.path.join(models_path, \"2D_GNN_MODEL.pth\")))\n",
    "model_2D = model_2D.to(\"cuda\")\n",
    "model_2D.eval()\n",
    "\n",
    "with open(os.path.join(data_path, \"means_and_stds.json\")) as f:\n",
    "    scaler = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c22fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "CHEMBL1833_5HT2B.csv\n",
      "============================================\n",
      "****************\n",
      "2D MODEL\n",
      "****************\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       408\n",
      "           1       0.81      0.50      0.62       134\n",
      "\n",
      "    accuracy                           0.85       542\n",
      "   macro avg       0.83      0.73      0.76       542\n",
      "weighted avg       0.84      0.85      0.83       542\n",
      "\n",
      "****************\n",
      "2D+3D MODEL\n",
      "*****************\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       408\n",
      "           1       0.90      0.28      0.43       134\n",
      "\n",
      "    accuracy                           0.82       542\n",
      "   macro avg       0.86      0.64      0.66       542\n",
      "weighted avg       0.83      0.82      0.78       542\n",
      "\n",
      "============================================\n",
      "CHEMBL214_5HT1A.csv\n",
      "============================================\n",
      "****************\n",
      "2D MODEL\n",
      "****************\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.71      0.73       549\n",
      "           1       0.77      0.80      0.79       674\n",
      "\n",
      "    accuracy                           0.76      1223\n",
      "   macro avg       0.76      0.76      0.76      1223\n",
      "weighted avg       0.76      0.76      0.76      1223\n",
      "\n",
      "****************\n",
      "2D+3D MODEL\n",
      "*****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:15:19] UFFTYPER: Warning: hybridization set to SP3 for atom 27\n",
      "[19:15:19] UFFTYPER: Warning: hybridization set to SP3 for atom 27\n",
      "[19:15:19] UFFTYPER: Warning: hybridization set to SP3 for atom 27\n",
      "[19:19:04] UFFTYPER: Warning: hybridization set to SP3 for atom 29\n",
      "[19:19:04] UFFTYPER: Warning: hybridization set to SP3 for atom 29\n",
      "[19:19:04] UFFTYPER: Warning: hybridization set to SP3 for atom 29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m*****************\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m X_train_3D = np.array([smiles_to_3D(smiles, fingerprint_model, scaler, \u001b[38;5;28;01mFalse\u001b[39;00m).detach().cpu() \u001b[38;5;28;01mfor\u001b[39;00m smiles \u001b[38;5;129;01min\u001b[39;00m X_train])\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m X_test_3D = np.array([\u001b[43msmiles_to_3D\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfingerprint_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.detach().cpu() \u001b[38;5;28;01mfor\u001b[39;00m smiles \u001b[38;5;129;01min\u001b[39;00m X_test])\n\u001b[32m     44\u001b[39m rows_all_nan_train = np.isnan(X_train_3D).all(axis=\u001b[32m1\u001b[39m)\n\u001b[32m     45\u001b[39m nan_indices_train = np.where(rows_all_nan_train)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\PycharmProjects\\mldd_project\\app\\hypotheses\\compare_with_other_fingerprints\\utils.py:215\u001b[39m, in \u001b[36msmiles_to_3D\u001b[39m\u001b[34m(smiles, model, scaler, has_pos)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msmiles_to_3D\u001b[39m(smiles, model, scaler, has_pos = \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m         _, nodes, edges = \u001b[43mcreate_df_from_mol\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m         record = build_graph(nodes, edges).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    217\u001b[39m         output = model(record)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\PycharmProjects\\mldd_project\\app\\hypotheses\\compare_with_other_fingerprints\\utils.py:207\u001b[39m, in \u001b[36mcreate_df_from_mol\u001b[39m\u001b[34m(rec, scaler, has_pos)\u001b[39m\n\u001b[32m    204\u001b[39m     mol = Chem.RemoveHs(mol)\n\u001b[32m    205\u001b[39m     node_features = [get_atom_features(atom) \u001b[38;5;28;01mfor\u001b[39;00m atom \u001b[38;5;129;01min\u001b[39;00m mol.GetAtoms()]\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m nodes, positions = \u001b[43mpreprocess_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m mol = add_conformer_to_mol(mol, positions)\n\u001b[32m    209\u001b[39m edges = get_edge_features(mol)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\PycharmProjects\\mldd_project\\app\\data_preprocessing\\utils.py:57\u001b[39m, in \u001b[36mpreprocess_df\u001b[39m\u001b[34m(df, positions, scaler)\u001b[39m\n\u001b[32m     54\u001b[39m     df[f] = df[f].clip(\u001b[32m0\u001b[39m, max_val) / max_val\n\u001b[32m     56\u001b[39m positions = normalize_df(positions, scaler).reset_index()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df, pd.DataFrame(positions, columns=[\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mz\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\frame.py:10412\u001b[39m, in \u001b[36mDataFrame.join\u001b[39m\u001b[34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[39m\n\u001b[32m  10402\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  10403\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[32m  10404\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10405\u001b[39m             other,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10410\u001b[39m             validate=validate,\n\u001b[32m  10411\u001b[39m         )\n\u001b[32m> \u001b[39m\u001b[32m10412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10413\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m  10418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m  10419\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlsuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrsuffix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10420\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10422\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  10423\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m  10424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:183\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    169\u001b[39m     op = _MergeOperation(\n\u001b[32m    170\u001b[39m         left_df,\n\u001b[32m    171\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m         validate=validate,\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:885\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    881\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m    883\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28mself\u001b[39m._get_join_info()\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:876\u001b[39m, in \u001b[36m_MergeOperation._reindex_and_concat\u001b[39m\u001b[34m(self, join_index, left_indexer, right_indexer, copy)\u001b[39m\n\u001b[32m    874\u001b[39m left.columns = llabels\n\u001b[32m    875\u001b[39m right.columns = rlabels\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m result = \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:393\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    378\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    380\u001b[39m op = _Concatenator(\n\u001b[32m    381\u001b[39m     objs,\n\u001b[32m    382\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     sort=sort,\n\u001b[32m    391\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    676\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    678\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    684\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m concat_axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     mgrs = \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[32m0\u001b[39m].concat_horizontal(mgrs, axes)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].nblocks > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[39m, in \u001b[36m_maybe_reindex_columns_na_proxy\u001b[39m\u001b[34m(axes, mgrs_indexers, needs_copy)\u001b[39m\n\u001b[32m    220\u001b[39m         mgr = mgr.reindex_indexer(\n\u001b[32m    221\u001b[39m             axes[i],\n\u001b[32m    222\u001b[39m             indexers[i],\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m             use_na_proxy=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[32m    228\u001b[39m         )\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         mgr = \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     new_mgrs.append(mgr)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:576\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    573\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    574\u001b[39m         new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcopy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m res.axes = new_axes\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    580\u001b[39m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:354\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    357\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:645\u001b[39m, in \u001b[36mBlock.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    643\u001b[39m refs: BlockValuesRefs | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m     values = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "folder = os.path.join(data_path, \"CHEMBL\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "scaler_path = \"2D_test_min_max.json\"\n",
    "\n",
    "for db in os.listdir(folder):\n",
    "    print(\"============================================\")\n",
    "    print(db)\n",
    "    print(\"============================================\")\n",
    "\n",
    "    data = os.path.join(folder, db)\n",
    "    X_train, X_test, y_train, y_test = read_data(data, 100)\n",
    "\n",
    "    X_train, y_train = list(X_train), list(y_train)\n",
    "    X_test, y_test = list(X_test), list(y_test)\n",
    "\n",
    "    ### 2D MODEL\n",
    "    print(\"****************\")\n",
    "    print(\"2D MODEL\")\n",
    "    print(\"****************\")\n",
    "    X_train_2D = np.array([smiles_to_2D_fp(smiles, model_2D, device, scaler_path).detach().cpu() for smiles in X_train])\n",
    "    X_test_2D = np.array([smiles_to_2D_fp(smiles, model_2D, device, scaler_path).detach().cpu() for smiles in X_test])\n",
    "\n",
    "    rows_all_nan_train = np.isnan(X_train_2D).all(axis=1)\n",
    "    nan_indices_train = np.where(rows_all_nan_train)[0]\n",
    "\n",
    "    rows_all_nan_test = np.isnan(X_test_2D).all(axis=1)\n",
    "    nan_indices_test = np.where(rows_all_nan_test)[0]\n",
    "\n",
    "    X_train_2D = np.delete(X_train_2D, nan_indices_train, axis=0)\n",
    "    y_train_2D = np.delete(y_train, nan_indices_train, axis=0)\n",
    "    X_test_2D = np.delete(X_test_2D, nan_indices_test, axis=0)\n",
    "    y_test_2D = np.delete(y_test, nan_indices_test, axis=0)\n",
    "\n",
    "    get_score(X_train_2D, y_train_2D, X_test_2D, y_test_2D)\n",
    "\n",
    "    ### 2D+3D\n",
    "    print(\"****************\")\n",
    "    print(\"2D+3D MODEL\")\n",
    "    print(\"*****************\")\n",
    "\n",
    "    X_train_3D = np.array([smiles_to_3D(smiles, fingerprint_model, scaler, False).detach().cpu() for smiles in X_train])\n",
    "    X_test_3D = np.array([smiles_to_3D(smiles, fingerprint_model, scaler, False).detach().cpu() for smiles in X_test])\n",
    "\n",
    "    rows_all_nan_train = np.isnan(X_train_3D).all(axis=1)\n",
    "    nan_indices_train = np.where(rows_all_nan_train)[0]\n",
    "\n",
    "    rows_all_nan_test = np.isnan(X_test_3D).all(axis=1)\n",
    "    nan_indices_test = np.where(rows_all_nan_test)[0]\n",
    "\n",
    "    X_train_3D = np.delete(X_train_3D, nan_indices_train, axis=0)\n",
    "    y_train_3D = np.delete(y_train, nan_indices_train, axis=0)\n",
    "    X_test_3D = np.delete(X_test_3D, nan_indices_test, axis=0)\n",
    "    y_test_3D = np.delete(y_test, nan_indices_test, axis=0)\n",
    "\n",
    "    get_score(X_train_3D, y_train_3D, X_test_3D, y_test_3D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7eac6",
   "metadata": {},
   "source": [
    "### 3D TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc0696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_3D(db_name, size):\n",
    "    path = os.path.join(data_path, db_name)\n",
    "    all_files = [ff for f in os.listdir(path) if os.path.isfile(ff := os.path.join(path, f))]\n",
    "\n",
    "    random_files = random.sample(all_files, size)\n",
    "    l = []\n",
    "    for i, file in enumerate(random_files):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        rec = {\"smiles\": data[\"smiles\"], \"conf\": data[\"atoms\"], \"homo\": data[\"homo\"], \"lumo\": data[\"lumo\"]}\n",
    "        for col in columns:\n",
    "            rec[col] = data[col]\n",
    "        l.append(rec)\n",
    "\n",
    "    return pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d257556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "mu\n",
      "============================================\n",
      "****************\n",
      "2D MODEL\n",
      "****************\n",
      "Regression Metrics:\n",
      "MAE:  0.2054\n",
      "RMSE: 0.4666\n",
      "RÂ²:   0.2717\n",
      "****************\n",
      "2D+3D MODEL\n",
      "*****************\n",
      "Regression Metrics:\n",
      "MAE:  0.1574\n",
      "RMSE: 0.3674\n",
      "RÂ²:   0.5483\n",
      "============================================\n",
      "zpve\n",
      "============================================\n",
      "****************\n",
      "2D MODEL\n",
      "****************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m2D MODEL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m****************\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m X_train_2D = np.array([\u001b[43msmiles_to_2D_fp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_2D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_path\u001b[49m\u001b[43m)\u001b[49m.detach().cpu() \u001b[38;5;28;01mfor\u001b[39;00m smiles \u001b[38;5;129;01min\u001b[39;00m X_train])\n\u001b[32m     23\u001b[39m X_test_2D = np.array([smiles_to_2D_fp(smiles[\u001b[32m0\u001b[39m], model_2D, device, scaler_path).detach().cpu() \u001b[38;5;28;01mfor\u001b[39;00m smiles \u001b[38;5;129;01min\u001b[39;00m X_test])\n\u001b[32m     25\u001b[39m rows_all_nan_train = np.isnan(X_train_2D).all(axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\PycharmProjects\\mldd_project\\app\\hypotheses\\3D_vs_2D_model\\model_2D\\utils.py:156\u001b[39m, in \u001b[36msmiles_to_2D_fp\u001b[39m\u001b[34m(smiles, model, device, scaler_path)\u001b[39m\n\u001b[32m    154\u001b[39m bond_min_vals, bond_max_vals = load_bond_stats(os.path.join(DATA_PATH, scaler_path))\n\u001b[32m    155\u001b[39m _, x, edge_index = create_df_from_mol(smiles, bond_min_vals, bond_max_vals)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m data = \u001b[43mData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m output = model(data)\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output.view(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\torch_geometric\\data\\data.py:362\u001b[39m, in \u001b[36mBaseData.to\u001b[39m\u001b[34m(self, device, non_blocking, *args)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], *args: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    358\u001b[39m        non_blocking: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    359\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[33;03m    only the ones given in :obj:`*args`.\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\torch_geometric\\data\\data.py:342\u001b[39m, in \u001b[36mBaseData.apply\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stores:\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\torch_geometric\\data\\storage.py:201\u001b[39m, in \u001b[36mBaseStorage.apply\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03mthe ones given in :obj:`*args`.\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items(*args):\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     \u001b[38;5;28mself\u001b[39m[key] = \u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\torch_geometric\\data\\storage.py:897\u001b[39m, in \u001b[36mrecursive_apply\u001b[39m\u001b[34m(data, func)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecursive_apply\u001b[39m(data: Any, func: Callable) -> Any:\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch.nn.utils.rnn.PackedSequence):\n\u001b[32m    899\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\torch_geometric\\data\\data.py:363\u001b[39m, in \u001b[36mBaseData.to.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], *args: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    358\u001b[39m        non_blocking: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    359\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[33;03m    only the ones given in :obj:`*args`.\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply(\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m, *args)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df = read_data_3D(\"qm9_data_json\", 20000)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for col in columns:\n",
    "    print(\"============================================\")\n",
    "    print(col)\n",
    "    print(\"============================================\")\n",
    "\n",
    "    X = df[[\"smiles\", \"conf\", \"homo\", \"lumo\"]]\n",
    "    y = df[col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    ### 2D MODEL\n",
    "    print(\"****************\")\n",
    "    print(\"2D MODEL\")\n",
    "    print(\"****************\")\n",
    "    \n",
    "    X_train_2D = np.array([smiles_to_2D_fp(smiles[0], model_2D, device, scaler_path).detach().cpu() for smiles in X_train])\n",
    "    X_test_2D = np.array([smiles_to_2D_fp(smiles[0], model_2D, device, scaler_path).detach().cpu() for smiles in X_test])\n",
    "\n",
    "    rows_all_nan_train = np.isnan(X_train_2D).all(axis=1)\n",
    "    nan_indices_train = np.where(rows_all_nan_train)[0]\n",
    "\n",
    "    rows_all_nan_test = np.isnan(X_test_2D).all(axis=1)\n",
    "    nan_indices_test = np.where(rows_all_nan_test)[0]\n",
    "\n",
    "    X_train_2D = np.delete(X_train_2D, nan_indices_train, axis=0)\n",
    "    y_train_2D = np.delete(y_train, nan_indices_train, axis=0)\n",
    "    X_test_2D = np.delete(X_test_2D, nan_indices_test, axis=0)\n",
    "    y_test_2D = np.delete(y_test, nan_indices_test, axis=0)\n",
    "\n",
    "    get_score(X_train_2D, y_train_2D, X_test_2D, y_test_2D, \"r\")\n",
    "\n",
    "    ### 2D+3D\n",
    "    print(\"****************\")\n",
    "    print(\"2D+3D MODEL\")\n",
    "    print(\"*****************\")\n",
    "\n",
    "    def get_3D_embedding(smiles, model, scaler):\n",
    "        return smiles_to_3D(smiles, model, scaler).detach().cpu()\n",
    "\n",
    "    embedding_fn_3D = partial(get_3D_embedding, model=fingerprint_model, scaler=scaler)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        X_train_3D = np.array(list(executor.map(embedding_fn_3D, X_train)))\n",
    "        X_test_3D = np.array(list(executor.map(embedding_fn_3D, X_test)))\n",
    "\n",
    "    rows_all_nan_train = np.isnan(X_train_3D).all(axis=1)\n",
    "    nan_indices_train = np.where(rows_all_nan_train)[0]\n",
    "\n",
    "    rows_all_nan_test = np.isnan(X_test_3D).all(axis=1)\n",
    "    nan_indices_test = np.where(rows_all_nan_test)[0]\n",
    "\n",
    "    X_train_3D = np.delete(X_train_3D, nan_indices_train, axis=0)\n",
    "    y_train_3D = np.delete(y_train, nan_indices_train, axis=0)\n",
    "    X_test_3D = np.delete(X_test_3D, nan_indices_test, axis=0)\n",
    "    y_test_3D = np.delete(y_test, nan_indices_test, axis=0)\n",
    "\n",
    "    get_score(X_train_3D, y_train_3D, X_test_3D, y_test_3D, \"r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldd25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
