{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7734dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error') \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from mol_level_model.model import DescriptorContrastiveModel\n",
    "from mol_level_model.utils import calculate_mean_and_std, approximate_homo_lumo\n",
    "\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from model.model import GNNFingerprint3D\n",
    "from hypotheses.compare_with_other_fingerprints.utils import smiles_to_3D\n",
    "from data_preprocessing.descriptors import calculate_descriptors_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "720bdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getenv(\"DATA_PATH\")\n",
    "models_path = os.getenv(\"MODELS_PATH\")\n",
    "\n",
    "columns = [\"mu\", \"zpve\", \"energy_U0\", \"rcA\", \"rcB\", \"rcC\", \"Cv\", \"alpha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Geometry import Point3D\n",
    "from rdkit.Chem import Conformer\n",
    "\n",
    "def attach_3D_coords_to_mol(mol, coords):\n",
    "    \"\"\"\n",
    "    Assigns 3D coordinates (list of [atom_symbol, x, y, z]) to mol.\n",
    "    \"\"\"\n",
    "    conf = Conformer(mol.GetNumAtoms())\n",
    "    for i, (symbol, x, y, z) in enumerate(coords):\n",
    "        conf.SetAtomPosition(i, Point3D(float(x), float(y), float(z)))\n",
    "\n",
    "    mol.AddConformer(conf, assignId=True)\n",
    "    return mol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4abffd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor(smiles, model, scaler, device, conf=None, homo=None, lumo=None):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    try:\n",
    "        mol = Chem.AddHs(mol)\n",
    "    except:\n",
    "        return torch.full((1024,), float('nan')).to(\"cuda\")\n",
    "     \n",
    "    if conf:\n",
    "        c = Conformer(mol.GetNumAtoms())\n",
    "        for i, (symbol, x, y, z) in enumerate(conf):\n",
    "            c.SetAtomPosition(i, Point3D(float(x), float(y), float(z)))\n",
    "        mol.AddConformer(c, assignId=True)\n",
    "    else:\n",
    "        result = AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "        if result != 0: return torch.full((1024,), float('nan')).to(device)\n",
    "        mol = Chem.RemoveHs(mol)\n",
    "\n",
    "    if not homo:\n",
    "        homo, lumo = approximate_homo_lumo(mol)\n",
    "\n",
    "    desc = calculate_descriptors_v2(smiles, mol, homo, lumo, False)\n",
    "    desc.pop(\"SMILES\", None)\n",
    "\n",
    "    for key in desc:\n",
    "        desc[key] = (desc[key] - scaler[key]['mean']) / scaler[key]['std']\n",
    "\n",
    "    rec = torch.tensor(list(desc.values()), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    pred = model(rec)\n",
    "    return pred.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ad731",
   "metadata": {},
   "source": [
    "### 2D TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd26fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, ki_threshold):\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    df['Activity'] = df['Standard Value'].apply(lambda x: 1 if x < ki_threshold else 0)\n",
    "\n",
    "    df = df[['Smiles', 'Activity']].dropna()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['Smiles'], df['Activity'], test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e090282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(X_train, y_train, X_test, y_test, task = \"c\"):\n",
    "    s_scaler = StandardScaler()\n",
    "    X_train = s_scaler.fit_transform(X_train)\n",
    "    X_test = s_scaler.transform(X_test)\n",
    "\n",
    "    # PCA to 167 dim\n",
    "    pca = PCA(n_components=167)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    if task == \"c\":\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    else:\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        print(\"Regression Metrics:\")\n",
    "        print(f\"MAE:  {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "        print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "        print(f\"R²:   {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5a6a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp\\ipykernel_24552\\1219052030.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fingerprint_model.load_state_dict(torch.load(os.path.join(models_path, \"GNN_MUCH_MORE_WEIGHT_3D.pth\")))\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp\\ipykernel_24552\\1219052030.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  descriptor_model.load_state_dict(torch.load(os.path.join(models_path, \"DESCRIPTOR_LEVEL_MODEL.pth\")))\n"
     ]
    }
   ],
   "source": [
    "fingerprint_model = GNNFingerprint3D(13, 5)\n",
    "fingerprint_model.load_state_dict(torch.load(os.path.join(models_path, \"GNN_MUCH_MORE_WEIGHT_3D.pth\")))\n",
    "fingerprint_model = fingerprint_model.to(\"cuda\")\n",
    "fingerprint_model.eval()\n",
    "\n",
    "descriptor_model = DescriptorContrastiveModel(input_dim=18)\n",
    "descriptor_model.load_state_dict(torch.load(os.path.join(models_path, \"DESCRIPTOR_LEVEL_MODEL.pth\")))\n",
    "descriptor_model = descriptor_model.to(\"cuda\")\n",
    "descriptor_model.eval()\n",
    "\n",
    "with open(os.path.join(data_path, \"means_and_stds.json\")) as f:\n",
    "    scaler = json.load(f)\n",
    "\n",
    "desc_scaler = calculate_mean_and_std(os.path.join(data_path, \"qm9_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b9777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "CHEMBL1833_5HT2B.csv\n",
      "============================================\n",
      "****************\n",
      "Mol level model\n",
      "****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2166/2166 [03:58<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90       408\n",
      "           1       0.78      0.46      0.58       134\n",
      "\n",
      "    accuracy                           0.83       542\n",
      "   macro avg       0.81      0.71      0.74       542\n",
      "weighted avg       0.83      0.83      0.82       542\n",
      "\n",
      "****************\n",
      "Atom level model\n",
      "*****************\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       408\n",
      "           1       0.91      0.31      0.46       134\n",
      "\n",
      "    accuracy                           0.82       542\n",
      "   macro avg       0.86      0.65      0.68       542\n",
      "weighted avg       0.84      0.82      0.79       542\n",
      "\n",
      "============================================\n",
      "CHEMBL214_5HT1A.csv\n",
      "============================================\n",
      "****************\n",
      "Mol level model\n",
      "****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3436/4891 [03:08<01:10, 20.75it/s][22:21:07] UFFTYPER: Warning: hybridization set to SP3 for atom 27\n",
      "[22:21:07] UFFTYPER: Warning: hybridization set to SP3 for atom 27\n",
      "100%|██████████| 4891/4891 [04:28<00:00, 18.25it/s]\n",
      "[22:22:53] UFFTYPER: Warning: hybridization set to SP3 for atom 29\n",
      "[22:22:53] UFFTYPER: Warning: hybridization set to SP3 for atom 29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m X_test_desc = np.delete(X_test_desc, nan_indices_test, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     31\u001b[39m y_test_desc = np.delete(y_test, nan_indices_test, axis=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mget_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_desc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m### 3D\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m****************\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mget_score\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, task)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# PCA to 167 dim\u001b[39;00m\n\u001b[32m      7\u001b[39m pca = PCA(n_components=\u001b[32m167\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m X_train = \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m X_test = pca.transform(X_test)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task == \u001b[33m\"\u001b[39m\u001b[33mc\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:468\u001b[39m, in \u001b[36mPCA.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    447\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[32m    448\u001b[39m \n\u001b[32m    449\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    466\u001b[39m \u001b[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     U, S, _, X, x_is_centered, xp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    470\u001b[39m         U = U[:, : \u001b[38;5;28mself\u001b[39m.n_components_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:544\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_full(X, n_components, xp, is_array_api_compliant)\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_truncated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:759\u001b[39m, in \u001b[36mPCA._fit_truncated\u001b[39m\u001b[34m(self, X, n_components, xp)\u001b[39m\n\u001b[32m    755\u001b[39m     U, Vt = svd_flip(U[:, ::-\u001b[32m1\u001b[39m], Vt[::-\u001b[32m1\u001b[39m], u_based_decision=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m svd_solver == \u001b[33m\"\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    758\u001b[39m     \u001b[38;5;66;03m# sign flipping is done inside\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m     U, S, Vt = \u001b[43mrandomized_svd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_centered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_oversamples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_oversamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterated_power\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m        \u001b[49m\u001b[43mflip_sign\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m     U, Vt = svd_flip(U, Vt, u_based_decision=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    770\u001b[39m \u001b[38;5;28mself\u001b[39m.n_samples_ = n_samples\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\utils\\extmath.py:524\u001b[39m, in \u001b[36mrandomized_svd\u001b[39m\u001b[34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[32m    521\u001b[39m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[32m    522\u001b[39m     M = M.T\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m Q = \u001b[43mrandomized_range_finder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[32m    533\u001b[39m B = Q.T @ M\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\utils\\extmath.py:335\u001b[39m, in \u001b[36mrandomized_range_finder\u001b[39m\u001b[34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# Perform power iterations with Q to further 'imprint' the top\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# singular vectors of A in Q\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     Q, _ = \u001b[43mnormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     Q, _ = normalizer(A.T @ Q)\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# Sample the range of A using by linear projection of Q\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# Extract an orthonormal basis\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\scipy\\linalg\\_decomp_lu.py:201\u001b[39m, in \u001b[36mlu\u001b[39m\u001b[34m(a, permute_l, overwrite_a, check_finite, p_indices)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33millegal value in \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33mth argument of internal gesv|posv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    198\u001b[39m                      % -info)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlu\u001b[39m(a, permute_l=\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a=\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    202\u001b[39m        p_indices=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    203\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03m    Compute LU decomposition of a matrix with partial pivoting.\u001b[39;00m\n\u001b[32m    205\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m \n\u001b[32m    296\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    297\u001b[39m     a1 = np.asarray_chkfinite(a) \u001b[38;5;28;01mif\u001b[39;00m check_finite \u001b[38;5;28;01melse\u001b[39;00m np.asarray(a)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "folder = os.path.join(data_path, \"CHEMBL\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for db in os.listdir(folder):\n",
    "    print(\"============================================\")\n",
    "    print(db)\n",
    "    print(\"============================================\")\n",
    "\n",
    "    data = os.path.join(folder, db)\n",
    "    X_train, X_test, y_train, y_test = read_data(data, 100)\n",
    "\n",
    "    X_train, y_train = list(X_train), list(y_train)\n",
    "    X_test, y_test = list(X_test), list(y_test)\n",
    "    \n",
    "    ### Descriptor\n",
    "    print(\"****************\")\n",
    "    print(\"Mol level model\")\n",
    "    print(\"****************\")\n",
    "    X_train_desc = np.array([get_descriptor(smiles, descriptor_model,desc_scaler, device).detach().cpu() for smiles in X_train])\n",
    "    X_test_desc = np.array([get_descriptor(smiles, descriptor_model,desc_scaler, device).detach().cpu() for smiles in X_test])\n",
    "\n",
    "    rows_all_nan_train = np.isnan(X_train_desc).all(axis=1)\n",
    "    nan_indices_train = np.where(rows_all_nan_train)[0]\n",
    "\n",
    "    rows_all_nan_test = np.isnan(X_test_desc).all(axis=1)\n",
    "    nan_indices_test = np.where(rows_all_nan_test)[0]\n",
    "\n",
    "    X_train_desc = np.delete(X_train_desc, nan_indices_train, axis=0)\n",
    "    y_train_desc = np.delete(y_train, nan_indices_train, axis=0)\n",
    "    X_test_desc = np.delete(X_test_desc, nan_indices_test, axis=0)\n",
    "    y_test_desc = np.delete(y_test, nan_indices_test, axis=0)\n",
    "\n",
    "    get_score(X_train_desc, y_train_desc, X_test_desc, y_test_desc)\n",
    "\n",
    "    ### 3D\n",
    "    print(\"****************\")\n",
    "    print(\"Atom level model\")\n",
    "    print(\"*****************\")\n",
    "\n",
    "    X_train_3D = np.array([smiles_to_3D(smiles, fingerprint_model, scaler, False).detach().cpu() for smiles in X_train])\n",
    "    X_test_3D = np.array([smiles_to_3D(smiles, fingerprint_model, scaler, False).detach().cpu() for smiles in X_test])\n",
    "\n",
    "    rows_all_nan_train = np.isnan(X_train_3D).all(axis=1)\n",
    "    nan_indices_train = np.where(rows_all_nan_train)[0]\n",
    "\n",
    "    rows_all_nan_test = np.isnan(X_test_3D).all(axis=1)\n",
    "    nan_indices_test = np.where(rows_all_nan_test)[0]\n",
    "\n",
    "    X_train_3D = np.delete(X_train_3D, nan_indices_train, axis=0)\n",
    "    y_train_3D = np.delete(y_train, nan_indices_train, axis=0)\n",
    "    X_test_3D = np.delete(X_test_3D, nan_indices_test, axis=0)\n",
    "    y_test_3D = np.delete(y_test, nan_indices_test, axis=0)\n",
    "\n",
    "    get_score(X_train_3D, y_train_3D, X_test_3D, y_test_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81fbd7",
   "metadata": {},
   "source": [
    "### 3D TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65967a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_3D(db_name, size):\n",
    "    path = os.path.join(data_path, db_name)\n",
    "    all_files = [ff for f in os.listdir(path) if os.path.isfile(ff := os.path.join(path, f))]\n",
    "\n",
    "    random_files = random.sample(all_files, size)\n",
    "    l = []\n",
    "    for i, file in enumerate(random_files):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        rec = {\"smiles\": data[\"smiles\"], \"conf\": data[\"atoms\"], \"homo\": data[\"homo\"], \"lumo\": data[\"lumo\"]}\n",
    "        for col in columns:\n",
    "            rec[col] = data[col]\n",
    "        l.append(rec)\n",
    "\n",
    "    return pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5de94e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "mu\n",
      "============================================\n",
      "****************\n",
      "Mol level model\n",
      "****************\n",
      "Regression Metrics:\n",
      "MAE:  0.1097\n",
      "RMSE: 0.5160\n",
      "R²:   0.1175\n",
      "****************\n",
      "Atom level model\n",
      "*****************\n",
      "Regression Metrics:\n",
      "MAE:  0.1666\n",
      "RMSE: 0.3772\n",
      "R²:   0.5286\n",
      "============================================\n",
      "zpve\n",
      "============================================\n",
      "****************\n",
      "Mol level model\n",
      "****************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m X_test_desc = np.delete(X_test_desc, nan_indices_test, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     33\u001b[39m y_test_desc = np.delete(y_test, nan_indices_test, axis=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mget_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m### 3D\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m****************\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mget_score\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, task)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     20\u001b[39m     model = RandomForestRegressor()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     y_pred = model.predict(X_test)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRegression Metrics:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    476\u001b[39m trees = [\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    479\u001b[39m ]\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[39m, in \u001b[36m_parallel_build_trees\u001b[39m\u001b[34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    187\u001b[39m         curr_sample_weight *= compute_sample_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, y, indices=indices)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m     tree._fit(\n\u001b[32m    198\u001b[39m         X,\n\u001b[32m    199\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m         missing_values_in_feature_mask=missing_values_in_feature_mask,\n\u001b[32m    203\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\anaconda3\\envs\\mldd25\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#df = read_data_3D(\"qm9_data_json\", 20000)\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for col in columns:\n",
    "    print(\"============================================\")\n",
    "    print(col)\n",
    "    print(\"============================================\")\n",
    "\n",
    "    X = df[[\"smiles\", \"conf\", \"homo\", \"lumo\"]]\n",
    "    y = df[col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    ### Descriptor\n",
    "    print(\"****************\")\n",
    "    print(\"Mol level model\")\n",
    "    print(\"****************\")\n",
    "    X_train_desc = np.array([get_descriptor(mol[0], descriptor_model, desc_scaler, device, mol[1], mol[2], mol[3]).detach().cpu() for mol in X_train])\n",
    "    X_test_desc = np.array([get_descriptor(mol[0], descriptor_model, desc_scaler, device, mol[1], mol[2], mol[3]).detach().cpu() for mol in X_test])\n",
    "\n",
    "    rows_all_nan_train = np.isnan(X_train_desc).all(axis=1)\n",
    "    nan_indices_train = np.where(rows_all_nan_train)[0]\n",
    "\n",
    "    rows_all_nan_test = np.isnan(X_test_desc).all(axis=1)\n",
    "    nan_indices_test = np.where(rows_all_nan_test)[0]\n",
    "\n",
    "    X_train_desc = np.delete(X_train_desc, nan_indices_train, axis=0)\n",
    "    y_train_desc = np.delete(y_train, nan_indices_train, axis=0)\n",
    "    X_test_desc = np.delete(X_test_desc, nan_indices_test, axis=0)\n",
    "    y_test_desc = np.delete(y_test, nan_indices_test, axis=0)\n",
    "\n",
    "    get_score(X_train_desc, y_train_desc, X_test_desc, y_test_desc, \"r\")\n",
    "\n",
    "    ### 3D\n",
    "    print(\"****************\")\n",
    "    print(\"Atom level model\")\n",
    "    print(\"*****************\")\n",
    "\n",
    "    X_train_3D = np.array([smiles_to_3D(smiles, fingerprint_model, scaler).detach().cpu() for smiles in X_train])\n",
    "    X_test_3D = np.array([smiles_to_3D(smiles, fingerprint_model, scaler).detach().cpu() for smiles in X_test])\n",
    "\n",
    "    rows_all_nan_train = np.isnan(X_train_3D).all(axis=1)\n",
    "    nan_indices_train = np.where(rows_all_nan_train)[0]\n",
    "\n",
    "    rows_all_nan_test = np.isnan(X_test_3D).all(axis=1)\n",
    "    nan_indices_test = np.where(rows_all_nan_test)[0]\n",
    "\n",
    "    X_train_3D = np.delete(X_train_3D, nan_indices_train, axis=0)\n",
    "    y_train_3D = np.delete(y_train, nan_indices_train, axis=0)\n",
    "    X_test_3D = np.delete(X_test_3D, nan_indices_test, axis=0)\n",
    "    y_test_3D = np.delete(y_test, nan_indices_test, axis=0)\n",
    "\n",
    "    get_score(X_train_3D, y_train_3D, X_test_3D, y_test_3D, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768302c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldd25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
