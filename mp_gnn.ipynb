{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e8183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset, Batch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeGraphDataset(Dataset):\n",
    "    def __init__(self, similar_mol_path, final_df, batch_size = 512):\n",
    "        super().__init__()\n",
    "        with open(similar_mol_path, \"r\") as f:\n",
    "            self.similar_mols = json.load(f)\n",
    "        self.mol_list = list(self.similar_mols.keys())\n",
    "        self.final_df = final_df\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mol_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor = self.mol_list[idx]\n",
    "        \n",
    "        for _ in range(20):\n",
    "            try:\n",
    "                positive = random.choice(self.similar_mols[anchor])\n",
    "                positive_data = self.final_df[positive]\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(f\"Failed to find valid positive for anchor: {anchor}\")\n",
    "\n",
    "        try:\n",
    "            anchor_data = self.final_df[anchor]\n",
    "        except:\n",
    "            if idx + 1 < len(self.mol_list):\n",
    "                anchor = self.mol_list[idx + 1]\n",
    "                anchor_data = self.final_df[anchor]\n",
    "            else:\n",
    "                raise IndexError(\"Anchor index out of range and fallback failed\")\n",
    "\n",
    "        negative_data = []\n",
    "        candidates = [mol for mol in self.mol_list if mol not in [anchor, positive]]\n",
    "        random.shuffle(candidates)\n",
    "        \n",
    "        for neg in candidates:\n",
    "            if len(negative_data) >= self.batch_size - 2:\n",
    "                break\n",
    "            try:\n",
    "                negative_data.append(self.final_df[neg])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if len(negative_data) < self.batch_size - 2:\n",
    "            raise ValueError(\"Not enough negative samples found\")\n",
    "\n",
    "        return anchor_data, positive_data, negative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f75bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "class GNNLayerSimple(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_dim + edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, node_dim)\n",
    "        )\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * node_dim + edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, edge_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        node_feat = torch.cat([h[row], edge_attr], dim=1)\n",
    "        delta_h = self.node_mlp(node_feat)\n",
    "\n",
    "        edge_feat = torch.cat([h[row], h[col], edge_attr], dim=1)\n",
    "        delta_edge = self.edge_mlp(edge_feat)\n",
    "\n",
    "        h = h + torch.zeros_like(h).scatter_add(0, row.unsqueeze(1).expand_as(delta_h), delta_h)\n",
    "        edge_attr = edge_attr + delta_edge\n",
    "        \n",
    "        return h, edge_attr\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Parameter(torch.randn(1, dim))  \n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "    def forward(self, h):\n",
    "        K = self.k_proj(h)  \n",
    "        V = self.v_proj(h)  \n",
    "        Q = self.q\n",
    "\n",
    "        scores = (Q @ K.T) * self.scale  \n",
    "        attn = F.softmax(scores, dim=-1) \n",
    "        out = attn @ V  \n",
    "\n",
    "        return out  \n",
    "\n",
    "class GNNFingerprintSimple(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim=256, num_layers=6, out_dim=1024):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.egnn_layers = nn.ModuleList([\n",
    "            GNNLayerSimple(node_dim, edge_dim, hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "        \n",
    "        self.attn_pool = AttentionPooling(node_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        t = False\n",
    "        if isinstance(data, list):\n",
    "            for graph in data:\n",
    "                if graph.edge_index.shape[0] != 2:\n",
    "                    graph.edge_index = graph.edge_index.t()\n",
    "            data = Batch.from_data_list(data)\n",
    "            t = True\n",
    "        else:\n",
    "            if data.edge_index.shape[0] != 2:\n",
    "                data.edge_index = data.edge_index.t()\n",
    "\n",
    "        h, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        for layer in self.egnn_layers:\n",
    "            h, edge_attr = layer(h, edge_index, edge_attr)\n",
    "\n",
    "        if t:\n",
    "            batch_size = data.num_graphs\n",
    "            pooled = torch.stack([ \n",
    "                self.attn_pool(h[data.batch == i]) for i in range(batch_size)\n",
    "            ])\n",
    "            pooled = pooled.squeeze(1)\n",
    "        else:\n",
    "            pooled = self.attn_pool(h)\n",
    "        \n",
    "        return self.projection_head(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bc46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntxent_loss(anchor_emb, positive_emb, negative_emb, temperature=0.1, margin=0.1):\n",
    "\n",
    "    positive_sim = F.cosine_similarity(anchor_emb, positive_emb, dim=-1)\n",
    "\n",
    "    negative_sim = F.cosine_similarity(anchor_emb.unsqueeze(1), negative_emb, dim=-1)\n",
    "\n",
    "    logits = torch.cat([positive_sim.unsqueeze(1), negative_sim], dim=1)\n",
    "    logits /= temperature\n",
    "\n",
    "    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n",
    "\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bf2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(data, model, model_name, optimizer, device, temperature=0.1, batch_size=512, epochs=100):\n",
    "    # Configure logging\n",
    "    log_dir = \"logging\"\n",
    "    log_filename = f\"{model_name}.log\"\n",
    "    log_filepath = os.path.join(log_dir, log_filename)\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Log file path: {log_filepath}\")\n",
    "    \n",
    "    # Logging settings\n",
    "    try:\n",
    "        logging.basicConfig(\n",
    "            filename=log_filepath,\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(message)s\",\n",
    "        )\n",
    "        logging.info(\"Logging initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while setting up logging: {e}\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"Logging has started...\")\n",
    "\n",
    "    logging.info(f\"Training {model_name}\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    logging.info(f\"Temperature: {temperature}, Batch Size: {batch_size}, Epochs: {epochs}\")\n",
    "\n",
    "    device = torch.device(device) \n",
    "    model = model.to(device)\n",
    "\n",
    "    n_batches = len(data) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        torch.cuda.empty_cache() \n",
    "        model.train() \n",
    "\n",
    "        total_loss = 0.0\n",
    "        with tqdm(total=n_batches, desc=f\"Epoch {epoch:02d}\", leave=False) as pbar:\n",
    "            for _ in range(n_batches):\n",
    "                idx = random.randint(0, len(data)-1)\n",
    "                anchor, positive, negative = data[idx]\n",
    "\n",
    "                anchor = anchor.to(device)\n",
    "                positive = positive.to(device)\n",
    "                negative = [neg.to(device) for neg in negative]\n",
    "\n",
    "                z_anchor = model(anchor)  \n",
    "                z_positive = model(positive) \n",
    "                z_negative = model(negative) \n",
    "    \n",
    "                loss = ntxent_loss(z_anchor, z_positive, z_negative, temperature)\n",
    "\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                #monitor_gradients(model)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "        logging.info(f\"Epoch {epoch} - Avg Train Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch} - Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model_save_dir = \"models\"\n",
    "    Path(model_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    model_save_path = os.path.join(model_save_dir, f\"{model_name}.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    logging.info(f\"Model saved to {model_save_path}\")\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    logging.info(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0deb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_mol_path = r\"data\\similar_mol.json\"\n",
    "\n",
    "with open(\"data\\\\final_df_dict.pkl\", \"rb\") as f:\n",
    "    final_df = pickle.load(f)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GNNFingerprintSimple(13, 5, 512, 8, 2048).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "temperature = .1\n",
    "batch_size = 512\n",
    "epochs = 50    \n",
    "data = MoleculeGraphDataset(similar_mol_path, final_df, batch_size= batch_size)\n",
    "\n",
    "training_loop(data, model, \"FINAL_GNN_NE_BIG_GREATER_OUTPUT\", optimizer, device, temperature, batch_size, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldd25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
